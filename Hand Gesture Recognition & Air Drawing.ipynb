{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand Gesture Recognition & Air Drawing\n",
    "\n",
    "## Overview:\n",
    "In this notebook we explore hand gesture (open/closed) recognition using computer vision along with a naive technique as well as two different machine learning techniques. We develop a system capable of recognizing hand gestures in real-time that is complemented with an \"air drawing\" functionality to draw in a board when the triggering gesture (closed hand) is recognized.\n",
    "\n",
    "Additionally, a nice functionality to generate labeled data for training on the fly is included.\n",
    "\n",
    "\n",
    "## Contents:\n",
    "1. [Hand gesture detection & Air Drawing: naive technique](#naive)\n",
    "2. ML models for gesture recognition: \n",
    "    1. [Hand gesture detection: clustering approach](#cluster) Unsupervised Learning with Clustering to detect whether hand is closed/open.\n",
    "    2. [Hand gesture detection: multilayer NN approach](#neuron) Supervised Learning by training a neural network (ANN) model to classify whether hand is closed/open. Also building a functionality to generate labeled data for training on the fly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Hand gesture detection & Air Drawing: naive technique](#naive)\n",
    "\n",
    "The technique involves calculating the distance between key landmarks, setting a manual threshold to determine hand open/close states. If the distance falls below the threshold, the hand is classified as closed; if it exceeds the threshold, the hand is classified as open. This is a simple and naive model that does have limitations such as e.g. if the hand is too far from the camera does method does not show any adaptation.\n",
    "\n",
    "In any case, the \"Air drawing\" feature is built and demostrated here. Drawing on the board happens with the movement of the hand, when closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "#Class for detecting hands\n",
    "\n",
    "class HandDetector:\n",
    "    def __init__(self, threshold=0.1,n_frames=5):\n",
    "        \"\"\"\n",
    "        Initializes HandDetector\n",
    "        \n",
    "        Args:\n",
    "        - threshold (float): Distance threshold for open/closed hand gesture detection.\n",
    "        - n_frames (int): Number of frames to use for averaging index finger position, which is the one serving as the pointer\n",
    "                          in the board. Introduced this idea to make it more robust to quick movements.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.hands = mp.solutions.hands.Hands() #MediaPipe hands module\n",
    "        self.mpDraw = mp.solutions.drawing_utils #initialize drawing module\n",
    "        self.threshold = threshold\n",
    "        self.index_start = None \n",
    "        self.mano_abierta = False \n",
    "        self.n_frames=n_frames\n",
    "        self.history = []\n",
    "    \n",
    "    def update_index_history(self, index_start):\n",
    "        \"\"\"\n",
    "        Updates history of index finger positions.\n",
    "\n",
    "        Args:\n",
    "        - index_start (mp.Vector2D): Position of index finger landmark.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.history.append(index_start) # adds new position\n",
    "        self.history = self.history[-self.n_frames:] # takes the last n_frames positions\n",
    "\n",
    "    def calculate_distance(self, point1, point2):\n",
    "        \"\"\"\n",
    "        Calculates Euclidean distance between two mediapipe coordinates\n",
    "\n",
    "        Args:\n",
    "        - point1: First point.\n",
    "        - point2: Second point.\n",
    "        \n",
    "        Returns:\n",
    "        - float: Euclidean distance between the points.\n",
    "        \n",
    "        \"\"\"\n",
    "        return math.sqrt((point1.x - point2.x)**2 + (point1.y - point2.y)**2)\n",
    "\n",
    "    def detect_hand(self, frame):\n",
    "        \"\"\"\n",
    "        Detects hands in the frame.\n",
    "\n",
    "        Args:\n",
    "        - frame: Input frame.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.index_start = None  # Reset index_start value for each iteration\n",
    "        results = self.hands.process(frame) # try to detect hands\n",
    "        # if hands are detected\n",
    "        if results.multi_hand_landmarks:\n",
    "            # draw landmarks on the frame\n",
    "            for handLms in results.multi_hand_landmarks:\n",
    "                self.mpDraw.draw_landmarks(frame, handLms, mp.solutions.hands.HAND_CONNECTIONS)\n",
    "\n",
    "                thumb_tip = handLms.landmark[4] # we take this as one reference\n",
    "                index_tip = handLms.landmark[8] # we take this as the other reference\n",
    "                self.index_start = handLms.landmark[5]  # update index_start\n",
    "\n",
    "                distance = self.calculate_distance(self.index_start, index_tip)\n",
    "                cv2.putText(frame, f\"Distance: {distance}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "                if distance < self.threshold: #then hand is closed\n",
    "                    self.mano_abierta = False\n",
    "                    cv2.putText(frame, f\"Closed hand\", (100, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                    self.update_index_history(self.index_start)\n",
    "                elif distance > self.threshold: #then hand is open\n",
    "                    self.mano_abierta = True\n",
    "                    self.history=[] #clean the index history to avoid corrupting the next piece of drawing\n",
    "                    cv2.putText(frame, f\"Open hand\", (100, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                    \n",
    "    \n",
    "    def calculate_average_index_position(self, history):\n",
    "        \"\"\"\n",
    "        Calculates average index finger position based on history.\n",
    "\n",
    "        Args:\n",
    "        - history (list): List of index finger positions.\n",
    "\n",
    "        Returns:\n",
    "        - list: Average x,y coordiates for index finger position.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        index_x_list=np.array([index_coord.x for index_coord in history])\n",
    "        index_y_list=np.array([index_coord.y for index_coord in history])\n",
    "        return [index_x_list.mean(), index_y_list.mean()]     \n",
    "        \n",
    "    def get_average_index_position(self):\n",
    "        \"\"\"\n",
    "        Retrieves average index finger position.\n",
    "\n",
    "        Returns:\n",
    "        - list: Average index finger position.\n",
    "        \n",
    "        \"\"\"\n",
    "        return self.calculate_average_index_position(self.history)\n",
    "\n",
    "    \n",
    "# Class for drawing\n",
    "class HandDrawer:\n",
    "    def __init__(self, drawing, reverse=True):\n",
    "        \"\"\"\n",
    "        Initializes HandDrawer\n",
    "\n",
    "        Args:\n",
    "        - drawing (numpy.ndarray): Initial drawing frame.\n",
    "        - reverse (bool): Flag to indicate if drawing is reversed.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.drawing = drawing\n",
    "        self.drawing_pointer = drawing\n",
    "        self.reverse = reverse\n",
    "\n",
    "    def draw_hand(self, average_index_position):\n",
    "        \"\"\"\n",
    "        Draws averaged index finger position on the drawing frame.\n",
    "\n",
    "        Args:\n",
    "        - average_index_position: Average position of index finger.\n",
    "        \n",
    "        \"\"\"\n",
    "        if average_index_position[0]>=0:\n",
    "            h, w, c = self.drawing.shape\n",
    "            cx, cy = int(average_index_position[0] * w), int(average_index_position[1] * h)\n",
    "            if self.reverse==False:\n",
    "                cv2.circle(self.drawing, (cx, cy), 10, (0, 0, 255), -1) #draw\n",
    "            if self.reverse == True:\n",
    "                cv2.circle(self.drawing, (w - cx, cy), 10, (0, 0, 255), -1) #draw\n",
    "    \n",
    "    def update_pointer(self, index_position):\n",
    "        \"\"\"\n",
    "        Updates drawing pointer position based on index finger coords.\n",
    "\n",
    "        Args:\n",
    "        - index_position: Position of index finger.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.drawing_pointer = self.drawing.copy()\n",
    "        h, w, c = self.drawing_pointer.shape\n",
    "        cx, cy = int(index_position.x * w), int(index_position.y * h)\n",
    "        if self.reverse==False:\n",
    "            cv2.circle(self.drawing_pointer, (cx, cy), 10, (0, 255, 255), -1)\n",
    "        if self.reverse == True:\n",
    "            cv2.circle(self.drawing_pointer, (w - cx, cy), 10, (0, 255, 255), -1)\n",
    "\n",
    "    def show_frames(self, original_frame):\n",
    "        \"\"\"\n",
    "        Displays original and drawing frames.\n",
    "\n",
    "        Args:\n",
    "        - original_frame (numpy.ndarray): Original frame.\n",
    "        \n",
    "        \"\"\"\n",
    "        cv2.imshow(\"original\", original_frame)\n",
    "        cv2.imshow(\"drawing\", self.drawing_pointer)\n",
    "\n",
    "# flips the frame horizontally     \n",
    "class Reversor:\n",
    "    def revert_frame(frame):\n",
    "        reverted_frame = frame[:, ::-1]\n",
    "        return reverted_frame\n",
    "\n",
    "def main():\n",
    "    # Capture video. Initialize before main loop\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    # Read the frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Initialize hand detector and drawer\n",
    "    hand_detector = HandDetector(n_frames=5)\n",
    "    hand_drawer = HandDrawer(np.zeros_like(frame))\n",
    "    \n",
    "    # Main loop\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        # Detect hand in the frame\n",
    "        hand_detector.detect_hand(frame)\n",
    "        \n",
    "        # Uncomment the following line if you want to reverse the frame\n",
    "        # frame = Reversor.revert_frame(frame)\n",
    "        \n",
    "        # Update pointer position if index finger detected\n",
    "        if hand_detector.index_start is not None:\n",
    "            index_position = hand_detector.index_start\n",
    "            hand_drawer.update_pointer(index_position)\n",
    "            \n",
    "        # Draw with hand movement if hand is closed and history is complete (for averaging)\n",
    "        if hand_detector.mano_abierta == False and len(hand_detector.history)==hand_detector.n_frames:\n",
    "            index_position = hand_detector.get_average_index_position()\n",
    "            hand_drawer.draw_hand(index_position)\n",
    "            \n",
    "        # Display both frames    \n",
    "        hand_drawer.show_frames(frame)\n",
    "\n",
    "        k = cv2.waitKey(1)\n",
    "        if k == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Hand gesture detection: clustering approach](#cluster)\n",
    "\n",
    "The technique involves applying a clustering algorithm (K-means with K = 2) to the extracted features from hand landmarks. We determine if the hand is open/closed depending on whether specific landmarks belong to the same cluster or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Open cap\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Initialize mediapipe\n",
    "hands = mp.solutions.hands.Hands()\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "# Initialize KMeans clustering with 2 clusters (to distinguish open/close)\n",
    "cluster = KMeans(n_clusters=2)\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    results = hands.process(frame)\n",
    "    \n",
    "    # If hands are detected\n",
    "    if results.multi_hand_landmarks:\n",
    "        # Draw landmarks on the frame\n",
    "        for handLms in results.multi_hand_landmarks:\n",
    "            mpDraw.draw_landmarks(frame, handLms, mp.solutions.hands.HAND_CONNECTIONS)\n",
    "            # Extract coordinates of hand landmarks\n",
    "            coord = np.array([[handLms.landmark[i].x, handLms.landmark[i].y] for i in range(len(handLms.landmark))])\n",
    "            #Apply K means \n",
    "            result = cluster.fit(coord)\n",
    "            \n",
    "            # Check the label of the 16th and 0th landmarks\n",
    "            # If they belong to the same cluster, consider the hand as closed\n",
    "            \n",
    "            if result.labels_[16] == result.labels_[0]:\n",
    "                cv2.putText(frame, f\"Closed hand\", (100, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            else:\n",
    "                cv2.putText(frame, f\"Open hand\", (100, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    \n",
    "    k = cv2.waitKey(1)\n",
    "    \n",
    "    if k == ord(\"q\"):\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Hand gesture detection: multilayer NN approach](#neuron)\n",
    "\n",
    "The unsupervised technique involves training an artificial neural network (ANN) to classify hand gestures as open or closed based on hand landmark coordinates. This supervised learning approach uses labeled training data that we generate on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from time import strftime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing Training Data on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_CLOSED = 0\n",
    "LABEL_OPEN = 1\n",
    "\n",
    "#Function to generate training data\n",
    "def get_data(data_size=1000):\n",
    "    \"\"\"\n",
    "    Capture hand landmark data for training for closed and open hand gestures.\n",
    "\n",
    "    Args:\n",
    "    - data_size (int): Size of the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - X (numpy.ndarray): Feature matrix containing hand landmark data.\n",
    "    - y (numpy.ndarray): Target vector containing labels for hand gestures.\n",
    "    \"\"\"\n",
    "\n",
    "    cap = cv2.VideoCapture(0) #open cap\n",
    "    #initialize mediapipe\n",
    "    hands = mp.solutions.hands.Hands()\n",
    "    mpDraw = mp.solutions.drawing_utils\n",
    "    \n",
    "    # where coords will be stored\n",
    "    coords_closed_hands=[]\n",
    "    coords_open_hands=[]\n",
    "\n",
    "\n",
    "    input(\"Please open your hand and press any key\")\n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        if len(coords_open_hands)==data_size/2: #take open hand coords until we have as many as half of the size of the data set\n",
    "            break\n",
    "        \n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        results = hands.process(frame)\n",
    "        \n",
    "        #if hands detected\n",
    "        if results.multi_hand_landmarks:\n",
    "            # draw landmarks\n",
    "            for handLms in results.multi_hand_landmarks:\n",
    "                mpDraw.draw_landmarks(frame, handLms, mp.solutions.hands.HAND_CONNECTIONS)\n",
    "                \n",
    "                # capture landmarks\n",
    "                coord = np.array([[handLms.landmark[i].x, handLms.landmark[i].y, handLms.landmark[i].z] for i in range(len(handLms.landmark))])\n",
    "                coord = coord.reshape(1,63)\n",
    "                coords_open_hands.append(coord) #add to the list\n",
    "                cv2.putText(frame, f\"Count:{len(coords_open_hands)+1}\", (100, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                \n",
    "                \n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "        k = cv2.waitKey(1)\n",
    "\n",
    "        if k == ord(\"q\"):\n",
    "            break\n",
    "            \n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    input(\"Please close your hand and press any key\")\n",
    "    \n",
    "    # Now same idea but for closed hand landmarks\n",
    "    while True:\n",
    "        \n",
    "        if len(coords_closed_hands)==data_size/2:\n",
    "            break\n",
    "        \n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        results = hands.process(frame)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for handLms in results.multi_hand_landmarks:\n",
    "                mpDraw.draw_landmarks(frame, handLms, mp.solutions.hands.HAND_CONNECTIONS)\n",
    "\n",
    "                coord = np.array([[handLms.landmark[i].x, handLms.landmark[i].y, handLms.landmark[i].z] for i in range(len(handLms.landmark))])\n",
    "                coord = coord.reshape(1,63)\n",
    "                coords_closed_hands.append(coord)\n",
    "                cv2.putText(frame, f\"Count:{len(coords_closed_hands)+1}\", (100, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                \n",
    "                \n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "        k = cv2.waitKey(1)\n",
    "\n",
    "        if k == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()\n",
    "    \n",
    "    closed_coords_X, open_coords_X = np.array(coords_closed_hands), np.array(coords_open_hands)\n",
    "    \n",
    "    # generate labels for each group of coords\n",
    "    closed_data_y = np.zeros((int(data_size/2),1), dtype=np.uint8)\n",
    "    open_data_y = np.ones((int(data_size/2),1), dtype=np.uint8)\n",
    "    \n",
    "    # concatenate feature matrices and target vectors\n",
    "    X = np.concatenate((closed_coords_X,open_coords_X), axis=0)\n",
    "    X = X.reshape(data_size, 63)\n",
    "    \n",
    "    y = np.concatenate((closed_data_y,open_data_y), axis=0)\n",
    "    \n",
    "    \n",
    "    print(\"Data succesfully captured!\")\n",
    "\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please open your hand and press any key\n",
      "Please close your hand and press any key\n",
      "Data succesfully captured!\n"
     ]
    }
   ],
   "source": [
    "DATA_SIZE = 500\n",
    "X,y = get_data(DATA_SIZE) # getting 250 closed hand coordinates and 250 open hand coordinates for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "# further split the training set into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the ANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a tensorboard callback\n",
    "\n",
    "def get_tensorboard(model_name):\n",
    "    folder_name = f'{model_name} at {strftime(\"%H %M\")}'\n",
    "    dir_paths = os.path.join(LOG_DIR, folder_name)\n",
    "\n",
    "    try:\n",
    "        os.makedirs(dir_paths)\n",
    "    except OSError as err:\n",
    "        print(err.strerror)\n",
    "    else:\n",
    "        print('Successfully created directory')\n",
    "    return TensorBoard(log_dir=dir_paths, histogram_freq=1)  \n",
    "\n",
    "LOG_DIR = 'tensorboard_logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created directory\n",
      "Epoch 1/20\n",
      "9/9 [==============================] - 1s 35ms/step - loss: 0.6565 - accuracy: 0.7761 - val_loss: 0.6136 - val_accuracy: 0.9851\n",
      "Epoch 2/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.5925 - accuracy: 0.9627 - val_loss: 0.5568 - val_accuracy: 0.9851\n",
      "Epoch 3/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.5354 - accuracy: 0.9664 - val_loss: 0.4943 - val_accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.4661 - accuracy: 1.0000 - val_loss: 0.4232 - val_accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3956 - accuracy: 1.0000 - val_loss: 0.3413 - val_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3101 - accuracy: 1.0000 - val_loss: 0.2594 - val_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2308 - accuracy: 1.0000 - val_loss: 0.1879 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1637 - accuracy: 1.0000 - val_loss: 0.1302 - val_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1124 - accuracy: 1.0000 - val_loss: 0.0898 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0778 - accuracy: 1.0000 - val_loss: 0.0611 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0550 - accuracy: 1.0000 - val_loss: 0.0554 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0441 - accuracy: 1.0000 - val_loss: 0.0327 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0303 - accuracy: 1.0000 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0235 - accuracy: 1.0000 - val_loss: 0.0201 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0188 - accuracy: 1.0000 - val_loss: 0.0156 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 0.0136 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.0109 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.0081 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.0079 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b980125be0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model architecture\n",
    "\n",
    "model_1 = Sequential([\n",
    "    Dense(units=128, input_dim=63, activation='relu', name='m1_hidden1'),\n",
    "    Dense(units=64, activation='relu', name='m1_hidden2'),\n",
    "    Dense(16, activation='relu', name='m1_hidden3'),\n",
    "    Dense(1, activation='sigmoid', name='m1_output') #sigmoid activation for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_1.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy', #for binary classification\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "nr_epochs = 20\n",
    "model_1.fit(X_train, y_train, epochs=nr_epochs,\n",
    "            callbacks=[get_tensorboard('model_1')],#tensorboard callback for visualization\n",
    "            verbose=1, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0123 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model_1.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created directory\n",
      "Epoch 1/20\n",
      "9/9 [==============================] - 1s 25ms/step - loss: 0.6898 - accuracy: 0.5522 - val_loss: 0.6736 - val_accuracy: 0.9104\n",
      "Epoch 2/20\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6728 - accuracy: 0.6269 - val_loss: 0.6498 - val_accuracy: 0.9403\n",
      "Epoch 3/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.6550 - accuracy: 0.7052 - val_loss: 0.6195 - val_accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.6275 - accuracy: 0.7500 - val_loss: 0.5821 - val_accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.6099 - accuracy: 0.7351 - val_loss: 0.5284 - val_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.5448 - accuracy: 0.8097 - val_loss: 0.4687 - val_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.5114 - accuracy: 0.8396 - val_loss: 0.4009 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.4576 - accuracy: 0.8097 - val_loss: 0.3328 - val_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.4081 - accuracy: 0.8582 - val_loss: 0.2668 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3766 - accuracy: 0.8358 - val_loss: 0.2186 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3290 - accuracy: 0.8507 - val_loss: 0.1720 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3412 - accuracy: 0.8694 - val_loss: 0.1407 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3300 - accuracy: 0.8545 - val_loss: 0.1208 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2828 - accuracy: 0.8731 - val_loss: 0.1137 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3027 - accuracy: 0.8843 - val_loss: 0.0969 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3267 - accuracy: 0.8545 - val_loss: 0.0945 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2985 - accuracy: 0.8731 - val_loss: 0.0845 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3163 - accuracy: 0.8582 - val_loss: 0.0779 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2879 - accuracy: 0.8955 - val_loss: 0.0819 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3175 - accuracy: 0.8657 - val_loss: 0.0985 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b9801de828>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model architecture with dropout layers. That's the only change wrt model_1\n",
    "\n",
    "model_drop = Sequential([\n",
    "    Dropout(0.2),\n",
    "    Dense(units=128, input_dim=63, activation='relu', name='m1_hidden1'),\n",
    "    Dropout(0.2),\n",
    "    Dense(units=64, activation='relu', name='m1_hidden2'),\n",
    "    Dense(16, activation='relu', name='m1_hidden3'),\n",
    "    Dense(1, activation='sigmoid', name='m1_output')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_drop.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "nr_epochs = 20\n",
    "model_drop.fit(X_train, y_train, epochs=nr_epochs,\n",
    "            callbacks=[get_tensorboard('model_drop')],\n",
    "            verbose=1, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the model to perform real-time detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detecting whether hand is open/closed with our ANN. \n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "hands = mp.solutions.hands.Hands()\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    results = hands.process(frame)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        for handLms in results.multi_hand_landmarks:\n",
    "            mpDraw.draw_landmarks(frame, handLms, mp.solutions.hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            #taking set of coordinates at the current moment\n",
    "            coord = np.array([[handLms.landmark[i].x, handLms.landmark[i].y, handLms.landmark[i].z] for i in range(len(handLms.landmark))])\n",
    "            coord = coord.reshape(1,63)\n",
    "            \n",
    "            # Predict whether hand is open or closed using the trained model\n",
    "            if float(model_1.predict(coord, verbose=0))<0.5: #the output comes from a sigmoid!\n",
    "                cv2.putText(frame, f\"Closed hand\", (100, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            else:\n",
    "                cv2.putText(frame, f\"Open hand\", (100, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    \n",
    "    k = cv2.waitKey(1)\n",
    "    \n",
    "    if k == ord(\"q\"):\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
